{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLrtyYAMsSsKHOxLA2SswJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms7039/Legal-Advice-Chatbot/blob/main/Mini_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'manyashrivastava'\n",
        "os.environ['KAGGLE_KEY'] = '6a059036a9090c8263edd6e2156ad2fc'\n",
        "\n",
        "# Then initialize the KaggleApi\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ],
      "metadata": {
        "id": "hoNTMGqz8kmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OhGw3zEM4vRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "import io\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import requests\n",
        "import pandas as pd\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "class LegalAdvisorChatbot:\n",
        "    def __init__(self, cache_dir=\"./legal_cache\"):\n",
        "        \"\"\"\n",
        "        Initialize the Legal Advisor Chatbot with Kaggle API integration\n",
        "        Args:\n",
        "            cache_dir (str): Directory to store cached data and downloads\n",
        "        \"\"\"\n",
        "        # Load NLP models\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "        # Kaggle API Configuration\n",
        "        self.kaggle_username = \"manyashrivastava\"\n",
        "        self.kaggle_key = \"6a059036a9090c8263edd6e2156ad2fc\"\n",
        "\n",
        "        # Legal datasets to use from Kaggle\n",
        "        self.legal_datasets = [\"akshatgupta7/llm-fine-tuning-dataset-of-indian-legal-texts\"]\n",
        "\n",
        "        # Cache configuration\n",
        "        self.cache_dir = cache_dir\n",
        "        self.cache_expiry = 30  # Cache expiry in days\n",
        "        self.ensure_cache_dir()\n",
        "\n",
        "        # Local fallback knowledge base\n",
        "        self.local_knowledge_path = os.path.join(cache_dir, \"local_knowledge_base.json\")\n",
        "        self.load_or_create_local_knowledge()\n",
        "\n",
        "        # Initialize Kaggle API\n",
        "        self.setup_kaggle_api()\n",
        "\n",
        "        # Cache frequently asked questions for better performance\n",
        "        self.cached_questions = list(self.local_knowledge.keys())\n",
        "\n",
        "        # Initialize the vectorizer before processing datasets\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(self.cached_questions)\n",
        "\n",
        "        # Download and process datasets if needed\n",
        "        self.dataset_path = os.path.join(cache_dir, \"datasets\")\n",
        "        self.ensure_datasets()\n",
        "\n",
        "        # Disclaimer message\n",
        "        self.disclaimer = (\n",
        "            \"IMPORTANT: This is an AI legal advisor and does not constitute professional legal advice. \"\n",
        "            \"Always consult with a qualified legal professional for specific legal guidance.\"\n",
        "        )\n",
        "\n",
        "    def setup_kaggle_api(self):\n",
        "        \"\"\"Set up Kaggle API authentication\"\"\"\n",
        "        # Create kaggle.json if it doesn't exist\n",
        "        kaggle_dir = os.path.expanduser('~/.kaggle')\n",
        "        if not os.path.exists(kaggle_dir):\n",
        "            os.makedirs(kaggle_dir)\n",
        "\n",
        "        kaggle_cred_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
        "        if not os.path.exists(kaggle_cred_path):\n",
        "            credentials = {\n",
        "                \"username\": self.kaggle_username,\n",
        "                \"key\": self.kaggle_key\n",
        "            }\n",
        "            with open(kaggle_cred_path, 'w') as f:\n",
        "                json.dump(credentials, f)\n",
        "            os.chmod(kaggle_cred_path, 0o600)  # Set appropriate permissions\n",
        "\n",
        "        # Initialize the API\n",
        "        self.kaggle_api = KaggleApi()\n",
        "        self.kaggle_api.authenticate()\n",
        "\n",
        "    def ensure_cache_dir(self):\n",
        "        \"\"\"Create cache directory if it doesn't exist\"\"\"\n",
        "        if not os.path.exists(self.cache_dir):\n",
        "            os.makedirs(self.cache_dir)\n",
        "        if not os.path.exists(os.path.join(self.cache_dir, \"datasets\")):\n",
        "            os.makedirs(os.path.join(self.cache_dir, \"datasets\"))\n",
        "\n",
        "    def ensure_datasets(self):\n",
        "        \"\"\"Download and extract datasets if not already available\"\"\"\n",
        "        # Check if datasets are already downloaded\n",
        "        dataset_index_path = os.path.join(self.cache_dir, \"dataset_index.json\")\n",
        "        if os.path.exists(dataset_index_path):\n",
        "            # Check if index is expired\n",
        "            file_time = os.path.getmtime(dataset_index_path)\n",
        "            file_datetime = datetime.fromtimestamp(file_time)\n",
        "            expiry_date = datetime.now() - timedelta(days=self.cache_expiry)\n",
        "            if file_datetime > expiry_date:\n",
        "                print(\"Using cached datasets\")\n",
        "                return\n",
        "\n",
        "        print(\"Downloading legal datasets from Kaggle...\")\n",
        "        dataset_index = {}\n",
        "\n",
        "        for dataset in self.legal_datasets:\n",
        "            try:\n",
        "                # Download the dataset\n",
        "                dataset_path = os.path.join(self.dataset_path, dataset.split('/')[1])\n",
        "                if not os.path.exists(dataset_path):\n",
        "                    os.makedirs(dataset_path)\n",
        "\n",
        "                print(f\"Downloading {dataset}...\")\n",
        "                self.kaggle_api.dataset_download_files(\n",
        "                    dataset,\n",
        "                    path=dataset_path,\n",
        "                    unzip=True\n",
        "                )\n",
        "\n",
        "                # Index the dataset contents\n",
        "                dataset_files = []\n",
        "                for root, _, files in os.walk(dataset_path):\n",
        "                    for file in files:\n",
        "                        if file.endswith('.csv') or file.endswith('.json'):\n",
        "                            rel_path = os.path.relpath(os.path.join(root, file), dataset_path)\n",
        "                            dataset_files.append(rel_path)\n",
        "\n",
        "                dataset_index[dataset] = {\n",
        "                    \"path\": dataset_path,\n",
        "                    \"files\": dataset_files\n",
        "                }\n",
        "\n",
        "                print(f\"Downloaded {dataset} successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading {dataset}: {e}\")\n",
        "\n",
        "        # Save the dataset index\n",
        "        with open(dataset_index_path, 'w') as f:\n",
        "            json.dump(dataset_index, f, indent=2)\n",
        "\n",
        "        # Process datasets to extract knowledge\n",
        "        self.process_datasets(dataset_index)\n",
        "\n",
        "    def process_datasets(self, dataset_index):\n",
        "        \"\"\"Process downloaded datasets and extract legal knowledge\"\"\"\n",
        "        print(\"Processing datasets to extract legal knowledge...\")\n",
        "        extracted_knowledge = {}\n",
        "\n",
        "        for dataset_name, dataset_info in dataset_index.items():\n",
        "            dataset_path = dataset_info[\"path\"]\n",
        "\n",
        "            for file_path in dataset_info[\"files\"]:\n",
        "                full_path = os.path.join(dataset_path, file_path)\n",
        "                try:\n",
        "                    if file_path.endswith('.csv'):\n",
        "                        # Process CSV files\n",
        "                        df = pd.read_csv(full_path)\n",
        "\n",
        "                        # Look for columns that might contain questions and answers\n",
        "                        if 'question' in df.columns and 'answer' in df.columns:\n",
        "                            for _, row in df.iterrows():\n",
        "                                extracted_knowledge[row['question']] = row['answer']\n",
        "\n",
        "                        # If legal articles dataset\n",
        "                        elif 'title' in df.columns and 'content' in df.columns:\n",
        "                            for _, row in df.iterrows():\n",
        "                                title = row['title']\n",
        "                                # Use title as a question\n",
        "                                question = f\"What is {title}?\" if not title.endswith('?') else title\n",
        "                                extracted_knowledge[question] = row['content'][:1000]  # Limit length\n",
        "\n",
        "                    elif file_path.endswith('.json'):\n",
        "                        # Process JSON files\n",
        "                        with open(full_path, 'r', encoding='utf-8') as f:\n",
        "                            try:\n",
        "                                data = json.load(f)\n",
        "\n",
        "                                # Process different JSON structures\n",
        "                                if isinstance(data, dict):\n",
        "                                    for key, value in data.items():\n",
        "                                        if isinstance(value, str) and len(value) > 50:\n",
        "                                            question = f\"What is {key}?\" if not key.endswith('?') else key\n",
        "                                            extracted_knowledge[question] = value[:1000]  # Limit length\n",
        "\n",
        "                                elif isinstance(data, list):\n",
        "                                    for item in data:\n",
        "                                        if isinstance(item, dict):\n",
        "                                            # Try to find question-answer pairs\n",
        "                                            q = item.get('question', item.get('title', ''))\n",
        "                                            a = item.get('answer', item.get('content', item.get('description', '')))\n",
        "\n",
        "                                            if q and a and len(a) > 50:\n",
        "                                                question = f\"What is {q}?\" if not q.endswith('?') else q\n",
        "                                                extracted_knowledge[question] = a[:1000]  # Limit length\n",
        "\n",
        "                            except json.JSONDecodeError:\n",
        "                                print(f\"Error decoding JSON file: {full_path}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {full_path}: {e}\")\n",
        "\n",
        "        # Merge with existing knowledge\n",
        "        self.local_knowledge.update(extracted_knowledge)\n",
        "\n",
        "        # Save updated knowledge base\n",
        "        with open(self.local_knowledge_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.local_knowledge, f, indent=2)\n",
        "\n",
        "        # Update cached questions and TF-IDF matrix\n",
        "        self.cached_questions = list(self.local_knowledge.keys())\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(self.cached_questions)\n",
        "\n",
        "        print(f\"Extracted {len(extracted_knowledge)} new legal knowledge items\")\n",
        "\n",
        "    def load_or_create_local_knowledge(self):\n",
        "        \"\"\"Load local knowledge base or create if it doesn't exist\"\"\"\n",
        "        if os.path.exists(self.local_knowledge_path):\n",
        "            try:\n",
        "                with open(self.local_knowledge_path, 'r', encoding='utf-8') as f:\n",
        "                    self.local_knowledge = json.load(f)\n",
        "            except (json.JSONDecodeError, UnicodeDecodeError):\n",
        "                print(\"Error reading local knowledge base. Creating new one.\")\n",
        "                self.create_default_knowledge_base()\n",
        "        else:\n",
        "            self.create_default_knowledge_base()\n",
        "\n",
        "    def create_default_knowledge_base(self):\n",
        "        \"\"\"Create a default knowledge base with basic legal information\"\"\"\n",
        "        self.local_knowledge = {\n",
        "            \"What is copyright?\": \"Copyright is a legal protection for original creative works, giving creators exclusive rights to use and distribute their work for a limited time.\",\n",
        "            \"How do I file for a trademark?\": \"To file for a trademark, you must submit an application to the USPTO, including proof of use and a detailed description of your mark.\",\n",
        "            \"What is fair use in copyright law?\": \"Fair use allows limited use of copyrighted material without permission for purposes like criticism, commentary, education, and research.\",\n",
        "            \"What constitutes patent infringement?\": \"Patent infringement occurs when someone makes, uses, sells, or imports a patented item or process without permission from the patent holder.\",\n",
        "            \"How long does copyright protection last?\": \"In the US, copyright generally lasts for the author's lifetime plus 70 years for works created after January 1, 1978.\",\n",
        "            \"What is the process for filing a civil lawsuit?\": \"The process typically involves: 1) Filing a complaint, 2) Serving the defendant, 3) Defendant's response, 4) Discovery phase, 5) Pre-trial motions, 6) Trial, and 7) Potential appeals.\",\n",
        "            \"What are the requirements for a valid contract?\": \"A valid contract requires: 1) Offer, 2) Acceptance, 3) Consideration (something of value exchanged), 4) Legal capacity of parties, 5) Legal purpose, and 6) Mutual agreement.\",\n",
        "            \"What is considered employment discrimination?\": \"Employment discrimination involves treating job applicants or employees unfavorably because of protected characteristics like race, color, religion, sex, national origin, age, disability, or genetic information.\",\n",
        "            \"What is the difference between a felony and misdemeanor?\": \"Felonies are more serious crimes punishable by imprisonment of more than one year or death. Misdemeanors are less serious offenses typically punishable by less than a year in jail.\",\n",
        "            \"How does bankruptcy protection work?\": \"Bankruptcy protection allows individuals or businesses to eliminate or repay debts under court protection. Common types include Chapter 7 (liquidation), Chapter 11 (reorganization), and Chapter 13 (adjustment of debts).\"\n",
        "        }\n",
        "\n",
        "        # Save the default knowledge base\n",
        "        with open(self.local_knowledge_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.local_knowledge, f, indent=2)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess input text using spaCy\n",
        "        Args:\n",
        "            text (str): Input text to preprocess\n",
        "        Returns:\n",
        "            str: Preprocessed text\n",
        "        \"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove punctuation and special characters\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        # Lemmatization and stopword removal\n",
        "        doc = self.nlp(text)\n",
        "        processed_tokens = [token.lemma_ for token in doc\n",
        "                           if not token.is_stop and token.is_alpha]\n",
        "\n",
        "        return ' '.join(processed_tokens)\n",
        "\n",
        "    def find_best_match(self, user_query):\n",
        "        \"\"\"\n",
        "        Find the best matching legal question using cosine similarity\n",
        "        Args:\n",
        "            user_query (str): User's input query\n",
        "        Returns:\n",
        "            tuple: Best matching question and its similarity score\n",
        "        \"\"\"\n",
        "        # Preprocess user query\n",
        "        processed_query = self.preprocess_text(user_query)\n",
        "\n",
        "        # Transform query to TF-IDF vector\n",
        "        query_vector = self.vectorizer.transform([processed_query])\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]\n",
        "\n",
        "        # Find best match\n",
        "        best_match_index = np.argmax(similarities)\n",
        "        best_match_score = similarities[best_match_index]\n",
        "\n",
        "        return (self.cached_questions[best_match_index], best_match_score)\n",
        "\n",
        "    def search_kaggle_datasets(self, query):\n",
        "        \"\"\"\n",
        "        Search Kaggle datasets for additional information about the query\n",
        "        Args:\n",
        "            query (str): Search query\n",
        "        Returns:\n",
        "            list: Results from Kaggle datasets\n",
        "        \"\"\"\n",
        "        # Get hash of query for cache filename\n",
        "        query_hash = str(hash(query) % 10000000)\n",
        "        cache_file = os.path.join(self.cache_dir, f\"kaggle_search_{query_hash}.json\")\n",
        "\n",
        "        # Check if we have a valid cached response\n",
        "        if os.path.exists(cache_file):\n",
        "            file_time = os.path.getmtime(cache_file)\n",
        "            file_datetime = datetime.fromtimestamp(file_time)\n",
        "            expiry_date = datetime.now() - timedelta(days=self.cache_expiry)\n",
        "\n",
        "            if file_datetime > expiry_date:\n",
        "                try:\n",
        "                    with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                        print(\"Using cached Kaggle search results\")\n",
        "                        return json.load(f)\n",
        "                except (json.JSONDecodeError, UnicodeDecodeError):\n",
        "                    print(\"Error reading cache file\")\n",
        "\n",
        "        print(f\"Searching Kaggle datasets for: {query}\")\n",
        "        results = []\n",
        "\n",
        "        # Search across all downloaded datasets\n",
        "        dataset_index_path = os.path.join(self.cache_dir, \"dataset_index.json\")\n",
        "        if os.path.exists(dataset_index_path):\n",
        "            with open(dataset_index_path, 'r') as f:\n",
        "                dataset_index = json.load(f)\n",
        "\n",
        "            # Extract key terms from query\n",
        "            doc = self.nlp(query)\n",
        "            search_terms = [token.text.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "            for dataset_name, dataset_info in dataset_index.items():\n",
        "                dataset_path = dataset_info[\"path\"]\n",
        "\n",
        "                for file_path in dataset_info[\"files\"]:\n",
        "                    full_path = os.path.join(dataset_path, file_path)\n",
        "\n",
        "                    try:\n",
        "                        if file_path.endswith('.csv'):\n",
        "                            # Search in CSV files\n",
        "                            df = pd.read_csv(full_path)\n",
        "\n",
        "                            # Convert all columns to string for searching\n",
        "                            text_df = df.select_dtypes(include=['object']).astype(str)\n",
        "\n",
        "                            # Search each column for matching terms\n",
        "                            for col in text_df.columns:\n",
        "                                for term in search_terms:\n",
        "                                    matches = text_df[text_df[col].str.contains(term, case=False, na=False)]\n",
        "\n",
        "                                    if not matches.empty:\n",
        "                                        for _, row in matches.iterrows():\n",
        "                                            result = {\n",
        "                                                \"dataset\": dataset_name,\n",
        "                                                \"file\": file_path,\n",
        "                                                \"match\": row.to_dict()\n",
        "                                            }\n",
        "                                            results.append(result)\n",
        "\n",
        "                                            # Limit to prevent overwhelming results\n",
        "                                            if len(results) >= 5:\n",
        "                                                break\n",
        "\n",
        "                        elif file_path.endswith('.json'):\n",
        "                            # Search in JSON files\n",
        "                            with open(full_path, 'r', encoding='utf-8') as f:\n",
        "                                try:\n",
        "                                    data = json.load(f)\n",
        "                                    json_str = json.dumps(data, ensure_ascii=False).lower()\n",
        "\n",
        "                                    # Check if any search term exists in the JSON\n",
        "                                    for term in search_terms:\n",
        "                                        if term.lower() in json_str:\n",
        "                                            # Add match\n",
        "                                            result = {\n",
        "                                                \"dataset\": dataset_name,\n",
        "                                                \"file\": file_path,\n",
        "                                                \"match\": {\"content\": \"JSON file contains relevant information\"}\n",
        "                                            }\n",
        "                                            results.append(result)\n",
        "                                            break\n",
        "\n",
        "                                except json.JSONDecodeError:\n",
        "                                    print(f\"Error decoding JSON file: {full_path}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error searching file {full_path}: {e}\")\n",
        "\n",
        "                    # Limit results\n",
        "                    if len(results) >= 5:\n",
        "                        break\n",
        "\n",
        "                if len(results) >= 5:\n",
        "                    break\n",
        "\n",
        "        # Cache the results\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def format_kaggle_results(self, results):\n",
        "        \"\"\"Format Kaggle search results into readable text\"\"\"\n",
        "        if not results:\n",
        "            return \"No additional information found in datasets.\"\n",
        "\n",
        "        formatted = \"Additional information from legal datasets:\\n\\n\"\n",
        "\n",
        "        for i, result in enumerate(results[:3], 1):  # Limit to top 3 results\n",
        "            dataset = result.get(\"dataset\", \"\").split(\"/\")[-1]\n",
        "            match = result.get(\"match\", {})\n",
        "\n",
        "            formatted += f\"{i}. From {dataset} dataset:\\n\"\n",
        "\n",
        "            # Format based on match content\n",
        "            if isinstance(match, dict):\n",
        "                # Extract the most relevant fields\n",
        "                relevant_fields = {}\n",
        "                for key, value in match.items():\n",
        "                    if isinstance(value, str) and len(value) > 5:\n",
        "                        if key.lower() in ['title', 'question', 'content', 'answer', 'summary', 'description']:\n",
        "                            relevant_fields[key] = value\n",
        "\n",
        "                # Format the relevant fields\n",
        "                for key, value in relevant_fields.items():\n",
        "                    if len(value) > 500:\n",
        "                        value = value[:500] + \"...\"\n",
        "                    formatted += f\"   {key.capitalize()}: {value}\\n\"\n",
        "\n",
        "            formatted += \"\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def update_local_knowledge(self, query, answer):\n",
        "        \"\"\"\n",
        "        Update local knowledge base with new information\n",
        "        Args:\n",
        "            query (str): The query\n",
        "            answer (str): The answer\n",
        "        \"\"\"\n",
        "        # Add to local knowledge if not already present\n",
        "        if query not in self.local_knowledge:\n",
        "            self.local_knowledge[query] = answer\n",
        "\n",
        "            # Update cached questions and TF-IDF matrix\n",
        "            self.cached_questions = list(self.local_knowledge.keys())\n",
        "            self.tfidf_matrix = self.vectorizer.fit_transform(self.cached_questions)\n",
        "\n",
        "            # Save updated knowledge base\n",
        "            with open(self.local_knowledge_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.local_knowledge, f, indent=2)\n",
        "\n",
        "    def get_response(self, user_query):\n",
        "        \"\"\"\n",
        "        Generate a response to the user's legal query\n",
        "        Args:\n",
        "            user_query (str): User's input query\n",
        "        Returns:\n",
        "            str: Appropriate legal advice or information\n",
        "        \"\"\"\n",
        "        # First check local knowledge base\n",
        "        best_match, similarity_score = self.find_best_match(user_query)\n",
        "\n",
        "        # If we have a good match in local knowledge\n",
        "        if similarity_score > 0.3:\n",
        "            local_answer = self.local_knowledge[best_match]\n",
        "\n",
        "            # If exact match, return immediately\n",
        "            if similarity_score > 0.8:\n",
        "                return f\"{local_answer}\\n\\n{self.disclaimer}\"\n",
        "\n",
        "            # If good but not exact match, search Kaggle too\n",
        "            kaggle_results = self.search_kaggle_datasets(user_query)\n",
        "\n",
        "            if kaggle_results:\n",
        "                formatted_kaggle = self.format_kaggle_results(kaggle_results)\n",
        "\n",
        "                if best_match != user_query:\n",
        "                    base_response = f\"Based on your query, I found this similar question:\\n\\n'{best_match}'\\n\\n{local_answer}\\n\\n\"\n",
        "                else:\n",
        "                    base_response = f\"{local_answer}\\n\\n\"\n",
        "\n",
        "                return f\"{base_response}---\\n{formatted_kaggle}\\n{self.disclaimer}\"\n",
        "            else:\n",
        "                if best_match != user_query:\n",
        "                    return f\"Based on your query, I found this similar question:\\n\\n'{best_match}'\\n\\n{local_answer}\\n\\n{self.disclaimer}\"\n",
        "                else:\n",
        "                    return f\"{local_answer}\\n\\n{self.disclaimer}\"\n",
        "        else:\n",
        "            # No good match in local knowledge, search Kaggle datasets\n",
        "            kaggle_results = self.search_kaggle_datasets(user_query)\n",
        "\n",
        "            if kaggle_results:\n",
        "                formatted_kaggle = self.format_kaggle_results(kaggle_results)\n",
        "\n",
        "                # Extract a potential answer from the first result\n",
        "                first_result = kaggle_results[0].get(\"match\", {})\n",
        "                potential_answer = \"\"\n",
        "\n",
        "                for key in [\"answer\", \"content\", \"description\", \"summary\"]:\n",
        "                    if key in first_result and isinstance(first_result[key], str):\n",
        "                        potential_answer = first_result[key]\n",
        "                        if len(potential_answer) > 1000:\n",
        "                            potential_answer = potential_answer[:1000] + \"...\"\n",
        "                        break\n",
        "\n",
        "                if potential_answer:\n",
        "                    # Add to local knowledge base for future queries\n",
        "                    self.update_local_knowledge(user_query, potential_answer)\n",
        "                    return f\"{potential_answer}\\n\\n---\\n{formatted_kaggle}\\n{self.disclaimer}\"\n",
        "                else:\n",
        "                    return f\"I found some information that might help with your query:\\n\\n{formatted_kaggle}\\n{self.disclaimer}\"\n",
        "            else:\n",
        "                # No information found\n",
        "                fallback_responses = [\n",
        "                    \"I'm sorry, but I couldn't find a precise match for your legal query in my knowledge base.\",\n",
        "                    \"Your query requires specialized legal knowledge that isn't in my database.\",\n",
        "                    \"I recommend consulting a legal professional for this specific issue, as I don't have enough information to provide guidance.\"\n",
        "                ]\n",
        "\n",
        "                return f\"{random.choice(fallback_responses)}\\n\\n{self.disclaimer}\"\n",
        "\n",
        "    def interactive_chat(self):\n",
        "        \"\"\"\n",
        "        Run an interactive chat session with the legal advisor\n",
        "        \"\"\"\n",
        "        print(\"Legal Advisor Chatbot: Hello! I can help answer general legal questions.\")\n",
        "        print(\"I use Kaggle datasets and a local knowledge base.\")\n",
        "        print(\"Type 'exit' to end the conversation.\")\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"\\nYour question: \")\n",
        "\n",
        "            if user_input.lower() == 'exit':\n",
        "                print(\"Thank you for using the Legal Advisor Chatbot. Stay informed!\")\n",
        "                break\n",
        "\n",
        "            response = self.get_response(user_input)\n",
        "            print(\"\\nLegal Advisor:\", response)\n",
        "\n",
        "    def refresh_datasets(self):\n",
        "        \"\"\"Force refresh of all datasets from Kaggle\"\"\"\n",
        "        # Remove the dataset index to force re-download\n",
        "        dataset_index_path = os.path.join(self.cache_dir, \"dataset_index.json\")\n",
        "        if os.path.exists(dataset_index_path):\n",
        "            os.remove(dataset_index_path)\n",
        "\n",
        "        # Re-download and process datasets\n",
        "        self.ensure_datasets()\n",
        "        print(\"Datasets refreshed successfully\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the chatbot with Kaggle API integration\n",
        "    legal_chatbot = LegalAdvisorChatbot()\n",
        "\n",
        "    # Uncomment to force refresh of datasets\n",
        "    # legal_chatbot.refresh_datasets()\n",
        "\n",
        "    legal_chatbot.interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ9rueEy2F7E",
        "outputId": "2f407e0e-ac3c-4238-a063-b57b4d8a5819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cached datasets\n",
            "Legal Advisor Chatbot: Hello! I can help answer general legal questions.\n",
            "I use Kaggle datasets and a local knowledge base.\n",
            "Type 'exit' to end the conversation.\n",
            "\n",
            "Your question: what is divorce\n",
            "Searching Kaggle datasets for: what is divorce\n",
            "\n",
            "Legal Advisor: Based on your query, I found this similar question:\n",
            "\n",
            "'What happens if a woman remarries after her divorce?'\n",
            "\n",
            "The magistrate cancels the previous order as from the date of her remarriage.\n",
            "\n",
            "---\n",
            "Additional information from legal datasets:\n",
            "\n",
            "1. From llm-fine-tuning-dataset-of-indian-legal-texts dataset:\n",
            "   Content: JSON file contains relevant information\n",
            "\n",
            "2. From llm-fine-tuning-dataset-of-indian-legal-texts dataset:\n",
            "   Content: JSON file contains relevant information\n",
            "\n",
            "\n",
            "IMPORTANT: This is an AI legal advisor and does not constitute professional legal advice. Always consult with a qualified legal professional for specific legal guidance.\n",
            "\n",
            "Your question: what is government\n",
            "Searching Kaggle datasets for: what is government\n",
            "\n",
            "Legal Advisor: Based on your query, I found this similar question:\n",
            "\n",
            "'In which cases does the 'appropriate Government' refer to the Central Government?'\n",
            "\n",
            "In cases where the sentence is for an offence against, or the order referred to in sub-section (6) is passed under, any law relating to a matter to which the executive power of the Union extends\n",
            "\n",
            "---\n",
            "Additional information from legal datasets:\n",
            "\n",
            "1. From llm-fine-tuning-dataset-of-indian-legal-texts dataset:\n",
            "   Content: JSON file contains relevant information\n",
            "\n",
            "2. From llm-fine-tuning-dataset-of-indian-legal-texts dataset:\n",
            "   Content: JSON file contains relevant information\n",
            "\n",
            "3. From llm-fine-tuning-dataset-of-indian-legal-texts dataset:\n",
            "   Content: JSON file contains relevant information\n",
            "\n",
            "\n",
            "IMPORTANT: This is an AI legal advisor and does not constitute professional legal advice. Always consult with a qualified legal professional for specific legal guidance.\n",
            "\n",
            "Your question: exit\n",
            "Thank you for using the Legal Advisor Chatbot. Stay informed!\n"
          ]
        }
      ]
    }
  ]
}